{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-74ba73874108>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import dateutil.tz\n",
    "import datetime\n",
    "import argparse\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "import metric\n",
    "import util\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "\n",
    "class clusGAN(object):\n",
    "    def __init__(self, g_net, d_net, enc_net, x_sampler, z_sampler, data, model, sampler,\n",
    "                 num_classes, dim_gen, n_cat, batch_size, beta_cycle_gen, beta_cycle_label):\n",
    "#定义\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.sampler = sampler\n",
    "        self.g_net = g_net\n",
    "        self.d_net = d_net\n",
    "        self.enc_net = enc_net\n",
    "        self.x_sampler = x_sampler\n",
    "        self.z_sampler = z_sampler\n",
    "        self.num_classes = num_classes\n",
    "        self.dim_gen = dim_gen\n",
    "        self.n_cat = n_cat\n",
    "        self.batch_size = batch_size\n",
    "        scale = 10.0\n",
    "        self.beta_cycle_gen = beta_cycle_gen\n",
    "        self.beta_cycle_label = beta_cycle_label\n",
    "\n",
    "        self.x_dim = self.d_net.x_dim\n",
    "        self.z_dim = self.g_net.z_dim\n",
    "\n",
    "#placeholder()函数是在神经网络构建graph的时候在模型中的占位，此时并没有把要输入的数据传入模型，它只会分配必要的内存。\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.x_dim], name='x')\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n",
    "#Z=(Zn,Zc)\n",
    "        self.z_gen = self.z[:,0:self.dim_gen]\n",
    "        self.z_hot = self.z[:,self.dim_gen:]\n",
    "#Generator 的输出\n",
    "        self.x_ = self.g_net(self.z)\n",
    "#encoder 的输出（3？）\n",
    "        self.z_enc_gen, self.z_enc_label, self.z_enc_logits = self.enc_net(self.x_, reuse=False)\n",
    "        self.z_infer_gen, self.z_infer_label, self.z_infer_logits = self.enc_net(self.x)\n",
    "\n",
    "#Discriminator的输出\n",
    "        self.d = self.d_net(self.x, reuse=False)\n",
    "        self.d_ = self.d_net(self.x_)\n",
    "\n",
    "#generator的loss(按照Algorithm2的更新策略计算loss)\n",
    "#tf.reduce_mean 函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值。\n",
    "#tf.nn.softmax_cross_entropy_with_logits求交叉熵的函数\n",
    "        self.g_loss = tf.reduce_mean(self.d_) + \\\n",
    "                      self.beta_cycle_gen * tf.reduce_mean(tf.square(self.z_gen - self.z_enc_gen)) +\\\n",
    "                      self.beta_cycle_label * tf.reduce_mean(\n",
    "                          tf.nn.softmax_cross_entropy_with_logits(logits=self.z_enc_logits,labels=self.z_hot))\n",
    "#discriminator的loss?\n",
    "        self.d_loss = tf.reduce_mean(self.d) - tf.reduce_mean(self.d_)\n",
    "    \n",
    "#tf.random_uniform([rows, colomns], maxval = high, minval = low, dtype = tf.float32)\n",
    "#返回一个维度为[rows, colomns]，范围为[low, high]的均匀分布随机浮点数张量\n",
    "\n",
    "        epsilon = tf.random_uniform([], 0.0, 1.0)\n",
    "# x_hat，d_hat epsilon是一个[0,1]的值，x中没有值；x_为generator的输出值    \n",
    "        x_hat = epsilon * self.x + (1 - epsilon) * self.x_\n",
    "        d_hat = self.d_net(x_hat)\n",
    "#tf.gradients()实现d_hat对x_hat求导,输出长度为 len(x_hat)\n",
    "        ddx = tf.gradients(d_hat, x_hat)[0]\n",
    "#reduce_sum() 用于计算张量tensor沿着某一维度的和，可以在求和后降维；若原2*3*4的tensor,axis=1,2*4\n",
    "        ddx = tf.sqrt(tf.reduce_sum(tf.square(ddx), axis=1))\n",
    "#?\n",
    "        ddx = tf.reduce_mean(tf.square(ddx - 1.0) * scale)\n",
    "#?\n",
    "        self.d_loss = self.d_loss + ddx\n",
    "#?\n",
    "        self.d_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9) \\\n",
    "                .minimize(self.d_loss, var_list=self.d_net.vars)\n",
    "        self.g_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9) \\\n",
    "                .minimize(self.g_loss, var_list=[self.g_net.vars, self.enc_net.vars])\n",
    "\n",
    "# ? Reconstruction Nodes\n",
    "        self.recon_loss = tf.reduce_mean(tf.abs(self.x - self.x_), 1)\n",
    "        self.compute_grad = tf.gradients(self.recon_loss, self.z)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        run_config = tf.ConfigProto()\n",
    "        run_config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "        run_config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=run_config)\n",
    "\n",
    "    def train(self, num_batches=100000):\n",
    "\n",
    "        now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "        timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        start_time = time.time()\n",
    "        print(\n",
    "        'Training {} on {}, sampler = {}, z = {} dimension, beta_n = {}, beta_c = {}'.\n",
    "            format(self.model, self.data, self.sampler, self.z_dim, self.beta_cycle_gen, self.beta_cycle_label))\n",
    "\n",
    "\n",
    "        for t in range(0, num_batches):\n",
    "\n",
    "            d_iters = 5\n",
    "\n",
    "            for _ in range(0, d_iters):\n",
    "                bx = self.x_sampler.train(batch_size)\n",
    "                bz = self.z_sampler(batch_size, self.z_dim, self.sampler, self.num_classes, self.n_cat)\n",
    "                #计算self.a_adam\n",
    "                self.sess.run(self.d_adam, feed_dict={self.x: bx, self.z: bz})\n",
    "\n",
    "            bz = self.z_sampler(batch_size, self.z_dim, self.sampler, self.num_classes, self.n_cat)\n",
    "             #计算self.g_adam\n",
    "            self.sess.run(self.g_adam, feed_dict={self.z: bz})\n",
    "\n",
    "            if (t+1) % 100 == 0:\n",
    "                bx = self.x_sampler.train(batch_size)\n",
    "                bz = self.z_sampler(batch_size, self.z_dim, self.sampler, self.num_classes, self.n_cat)\n",
    "                #计算d_loss\n",
    "                d_loss = self.sess.run(\n",
    "                    self.d_loss, feed_dict={self.x: bx, self.z: bz}\n",
    "                )\n",
    "                g_loss = self.sess.run(\n",
    "                    self.g_loss, feed_dict={self.z: bz}\n",
    "                )\n",
    "                print('Iter [%8d] Time [%5.4f] d_loss [%.4f] g_loss [%.4f]' %\n",
    "                      (t+1, time.time() - start_time, d_loss, g_loss))\n",
    "\n",
    "        self.recon_enc(timestamp, val=True)\n",
    "        self.save(timestamp)\n",
    "\n",
    "    def save(self, timestamp):\n",
    "\n",
    "        checkpoint_dir = 'checkpoint_dir/{}/{}_{}_{}_z{}_cyc{}_gen{}'.format(self.data, timestamp, self.model, self.sampler,\n",
    "                                                                             self.z_dim, self.beta_cycle_label,\n",
    "                                                                             self.beta_cycle_gen)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, 'model.ckpt'))\n",
    "\n",
    "    def load(self, pre_trained = False, timestamp = ''):\n",
    "\n",
    "        if pre_trained == True:\n",
    "            print('Loading Pre-trained Model...')\n",
    "            checkpoint_dir = 'pre_trained_models/{}/{}_{}_z{}_cyc{}_gen{}'.format(self.data, self.model, self.sampler,\n",
    "                                                                            self.z_dim, self.beta_cycle_label, self.beta_cycle_gen)\n",
    "        else:\n",
    "            if timestamp == '':\n",
    "                print('Best Timestamp not provided. Abort !')\n",
    "                checkpoint_dir = ''\n",
    "            else:\n",
    "                checkpoint_dir = 'checkpoint_dir/{}/{}_{}_{}_z{}_cyc{}_gen{}'.format(self.data, timestamp, self.model, self.sampler,\n",
    "                                                                                     self.z_dim, self.beta_cycle_label,\n",
    "                                                                                     self.beta_cycle_gen)\n",
    "\n",
    "\n",
    "        self.saver.restore(self.sess, os.path.join(checkpoint_dir, 'model.ckpt'))\n",
    "        print('Restored model weights.')\n",
    "\n",
    "\n",
    "    def _gen_samples(self, num_samples):\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        bz = self.z_sampler(batch_size, self.z_dim, self.sampler, self.num_classes, self.n_cat)  \n",
    "        fake_samples = self.sess.run(self.x_, feed_dict = {self.z : bz})\n",
    "        for t in range(num_samples // batch_size):\n",
    "            bz = self.z_sampler(batch_size, self.z_dim, self.sampler, self.num_classes, self.n_cat)\n",
    "            samp = self.sess.run(self.x_, feed_dict = {self.z : bz})\n",
    "            fake_samples = np.vstack((fake_samples, samp))\n",
    "\n",
    "        print(' Generated {} samples .'.format(fake_samples.shape[0]))\n",
    "        np.save('./Image_samples/{}/{}_{}_K_{}_gen_images.npy'.format(self.data, self.model, self.sampler, self.num_classes), fake_samples)\n",
    "\n",
    "    def recon_enc(self, timestamp, val = True):\n",
    "\n",
    "        if val:\n",
    "            data_recon, label_recon = self.x_sampler.validation()\n",
    "        else:\n",
    "            data_recon, label_recon = self.x_sampler.test()\n",
    "            #data_recon, label_recon = self.x_sampler.load_all()\n",
    "\n",
    "        num_pts_to_plot = data_recon.shape[0]\n",
    "        recon_batch_size = self.batch_size\n",
    "        latent = np.zeros(shape=(num_pts_to_plot, self.z_dim))\n",
    "\n",
    "        print('Data Shape = {}, Labels Shape = {}'.format(data_recon.shape, label_recon.shape))\n",
    "        for b in range(int(np.ceil(num_pts_to_plot*1.0 / recon_batch_size))):\n",
    "\n",
    "            if (b+1)*recon_batch_size > num_pts_to_plot:\n",
    "               pt_indx = np.arange(b*recon_batch_size, num_pts_to_plot)\n",
    "            else:\n",
    "               pt_indx = np.arange(b*recon_batch_size, (b+1)*recon_batch_size)\n",
    "            xtrue = data_recon[pt_indx, :]\n",
    "\n",
    "            zhats_gen, zhats_label = self.sess.run([self.z_infer_gen, self.z_infer_label], feed_dict={self.x : xtrue})\n",
    "\n",
    "            latent[pt_indx, :] = np.concatenate((zhats_gen, zhats_label), axis=1)\n",
    "\n",
    "        if self.beta_cycle_gen == 0:\n",
    "            self._eval_cluster(latent[:, self.dim_gen:], label_recon, timestamp, val)\n",
    "        else:\n",
    "            self._eval_cluster(latent, label_recon, timestamp, val)\n",
    "\n",
    "\n",
    "    def _eval_cluster(self, latent_rep, labels_true, timestamp, val):\n",
    "\n",
    "        map_labels = {0: 0, 1: 1, 2: 2, 4: 3, 6: 4, 7: 5, 8: 6, 9: 7}\n",
    "        labels_true = np.array([map_labels[i] for i in labels_true])\n",
    "\n",
    "        km = KMeans(n_clusters=max(self.num_classes, len(np.unique(labels_true))), random_state=0).fit(latent_rep)\n",
    "        labels_pred = km.labels_\n",
    "\n",
    "        purity = metric.compute_purity(labels_pred, labels_true)\n",
    "        ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "        nmi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "        if val:\n",
    "            data_split = 'Validation'\n",
    "        else:\n",
    "            data_split = 'Test'\n",
    "            #data_split = 'All'\n",
    "\n",
    "        print('Data = {}, Model = {}, sampler = {}, z_dim = {}, beta_label = {}, beta_gen = {} '\n",
    "              .format(self.data, self.model, self.sampler, self.z_dim, self.beta_cycle_label, self.beta_cycle_gen))\n",
    "        print(' #Points = {}, K = {}, Purity = {},  NMI = {}, ARI = {},  '\n",
    "              .format(latent_rep.shape[0], self.num_classes, purity, nmi, ari))\n",
    "\n",
    "        with open('logs/Res_{}_{}.txt'.format(self.data, self.model), 'a+') as f:\n",
    "                f.write('{}, {} : K = {}, z_dim = {}, beta_label = {}, beta_gen = {}, sampler = {}, Purity = {}, NMI = {}, ARI = {}\\n'\n",
    "                        .format(timestamp, data_split, self.num_classes, self.z_dim, self.beta_cycle_label, self.beta_cycle_gen,\n",
    "                                self.sampler, purity, nmi, ari))\n",
    "                f.flush()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('')\n",
    "    parser.add_argument('--data', type=str, default='10x_73k')\n",
    "    parser.add_argument('--model', type=str, default='clus_wgan')\n",
    "    parser.add_argument('--sampler', type=str, default='one_hot')\n",
    "    parser.add_argument('--K', type=int, default=8)\n",
    "    parser.add_argument('--dz', type=int, default=30)\n",
    "    parser.add_argument('--bs', type=int, default=64)\n",
    "    parser.add_argument('--beta_n', type=float, default=10.0)\n",
    "    parser.add_argument('--beta_c', type=float, default=10.0)\n",
    "    parser.add_argument('--timestamp', type=str, default='')\n",
    "    parser.add_argument('--train', type=str, default='False')\n",
    "    args = parser.parse_args()\n",
    "    data = importlib.import_module(args.data)\n",
    "    model = importlib.import_module(args.data + '.' + args.model)\n",
    "\n",
    "    num_classes = args.K\n",
    "    dim_gen = args.dz\n",
    "    n_cat = 1\n",
    "    batch_size = args.bs\n",
    "    beta_cycle_gen = args.beta_n\n",
    "    beta_cycle_label = args.beta_c\n",
    "    timestamp = args.timestamp\n",
    "\n",
    "    z_dim = dim_gen + num_classes * n_cat\n",
    "    d_net = model.Discriminator()\n",
    "    g_net = model.Generator(z_dim=z_dim)\n",
    "    enc_net = model.Encoder(z_dim=z_dim, dim_gen = dim_gen)\n",
    "    xs = data.DataSampler()\n",
    "    zs = util.sample_Z\n",
    "\n",
    "    cl_gan = clusGAN(g_net, d_net, enc_net, xs, zs, args.data, args.model, args.sampler,\n",
    "                     num_classes, dim_gen, n_cat, batch_size, beta_cycle_gen, beta_cycle_label)\n",
    "\n",
    "    if args.train == 'True':\n",
    "        cl_gan.train()\n",
    "    else:\n",
    "\n",
    "        print('Attempting to Restore Model ...')\n",
    "        if timestamp == '':\n",
    "            cl_gan.load(pre_trained=True)\n",
    "            timestamp = 'pre-trained'\n",
    "        else:\n",
    "            cl_gan.load(pre_trained=False, timestamp = timestamp)\n",
    "\n",
    "        cl_gan.recon_enc(timestamp, val=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
